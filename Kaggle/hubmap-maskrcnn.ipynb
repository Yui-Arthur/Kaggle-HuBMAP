{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random\nfrom pathlib import Path\nimport math\nimport cv2\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:05:17.903830Z","iopub.execute_input":"2023-06-17T13:05:17.904470Z","iopub.status.idle":"2023-06-17T13:05:22.450553Z","shell.execute_reply.started":"2023-06-17T13:05:17.904435Z","shell.execute_reply":"2023-06-17T13:05:22.449617Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pytorch/vision.git\n%cd vision\n# !git checkout v0.8.2\n\n!cp references/detection/utils.py ../\n!cp references/detection/transforms.py ../\n!cp references/detection/coco_eval.py ../\n!cp references/detection/engine.py ../\n!cp references/detection/coco_utils.py ../\n\n%cd ..\n!rm -r vision\n!pip install pycocotools\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:05:22.452704Z","iopub.execute_input":"2023-06-17T13:05:22.453629Z","iopub.status.idle":"2023-06-17T13:07:16.158322Z","shell.execute_reply.started":"2023-06-17T13:05:22.453594Z","shell.execute_reply":"2023-06-17T13:07:16.157164Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'vision'...\nremote: Enumerating objects: 339089, done.\u001b[K\nremote: Counting objects: 100% (55163/55163), done.\u001b[K\nremote: Compressing objects: 100% (947/947), done.\u001b[K\nremote: Total 339089 (delta 54375), reused 54837 (delta 54185), pack-reused 283926\u001b[K\nReceiving objects: 100% (339089/339089), 680.82 MiB | 29.24 MiB/s, done.\nResolving deltas: 100% (312432/312432), done.\n/kaggle/working/vision\n/kaggle/working\nCollecting pycocotools\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.6.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.23.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=93512 sha256=e3468c31ab40eb7a7d8b6a9358b1b501b928897fc28feab9b0751589312d4a32\n  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\nSuccessfully built pycocotools\nInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n    # return model\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:16.160670Z","iopub.execute_input":"2023-06-17T13:07:16.161042Z","iopub.status.idle":"2023-06-17T13:07:16.169370Z","shell.execute_reply.started":"2023-06-17T13:07:16.161000Z","shell.execute_reply":"2023-06-17T13:07:16.168284Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import transforms as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.PILToTensor())\n    transforms.append(T.ConvertImageDtype(torch.float))\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n#         transforms.append(T.RandomZoomOut(side_range = (1,1.5)))\n        transforms.append(T.RandomIoUCrop())\n#         transforms.append(T.RandomPhotometricDistort())\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:16.172703Z","iopub.execute_input":"2023-06-17T13:07:16.173418Z","iopub.status.idle":"2023-06-17T13:07:16.193085Z","shell.execute_reply.started":"2023-06-17T13:07:16.173386Z","shell.execute_reply":"2023-06-17T13:07:16.192007Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_valid_data(labels):\n    valid_data = []\n    for i in range(len(labels)):\n        for j in range(len(labels.iloc[i]['annotations'])):\n            if labels.iloc[i]['annotations'][j]['type'] == \"blood_vessel\":\n                valid_data.append(labels.iloc[i]['id'])\n                break\n    \n    return valid_data\n\ndef gen_mask(annotations):\n    boxes = []\n    masks = []\n    # area = []\n    for i in range(len(annotations)):\n\n        if annotations[i]['type'] == \"blood_vessel\":\n            pts = np.array(annotations[i]['coordinates'])\n            # print(pts[0][0])\n            min_xy = np.min(pts, axis=1)[0]\n            max_xy = np.max(pts, axis=1)[0]\n\n            boxes += [np.concatenate((min_xy, max_xy), axis=0)]\n\n            mask = np.zeros((512,512), dtype=np.uint8)\n            cv2.fillPoly(mask, pts, 1)\n\n            # area.append((max_xy[0] - min_xy[0]) * (max_xy[1] - min_xy[1]))\n\n            masks += [mask]\n        # break\n    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n    masks = torch.as_tensor(masks, dtype=torch.uint8)\n    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n    return boxes , masks , area","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:16.194797Z","iopub.execute_input":"2023-06-17T13:07:16.195448Z","iopub.status.idle":"2023-06-17T13:07:16.205975Z","shell.execute_reply.started":"2023-06-17T13:07:16.195416Z","shell.execute_reply":"2023-06-17T13:07:16.204997Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class KidneyDataset(Dataset):\n    def __init__(self , labels , metadata , image_list , tfm):\n        self.labels = labels\n        self.matadata = metadata\n        self.image_list = image_list\n        self.tfm = tfm\n        \n        self.image_with_target = {}\n\n        image_path = Path(image_list[0]).parent\n        print(image_path)\n        \n        for idx , col in labels.iterrows():\n            \n            # if f\"{image_path}\\\\{col['id']}.tif\" in image_list:\n            if f\"{image_path}/{col['id']}.tif\" in image_list:\n                polygons = col['annotations']\n                # print(polygons[0])\n                boxes , masks , area = gen_mask(polygons)\n                iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n                \n                target = {}\n                target[\"boxes\"] = boxes\n                target[\"labels\"] = torch.ones((len(boxes),), dtype=torch.int64)\n                target[\"masks\"] = masks\n                target[\"image_id\"] = torch.tensor([idx])\n                target[\"area\"] = area\n                target[\"iscrowd\"] = iscrowd\n                self.image_with_target[col['id']] = target\n\n        # print()\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self,idx):\n        \n        # id = self.image_list[idx].split('\\\\')[-1].split('.')[0]\n        id = self.image_list[idx].split('/')[-1].split('.')[0]\n        img = Image.open(self.image_list[idx])\n        target = self.image_with_target[id]\n\n\n        if self.tfm is not None:\n            img, target = self.tfm(img, target)\n        # print(target)\n        return img , target\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:16.208097Z","iopub.execute_input":"2023-06-17T13:07:16.208761Z","iopub.status.idle":"2023-06-17T13:07:16.220025Z","shell.execute_reply.started":"2023-06-17T13:07:16.208727Z","shell.execute_reply":"2023-06-17T13:07:16.218940Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef draw_fig(train , valid , name):\n    print(train)\n    print(valid)\n    \n    \n    if valid != None:\n        plt.plot([*range(1,len(train)+1)] , train , label = \"bbox\")\n        plt.plot([*range(1,len(valid)+1)] , valid , label = \"segm\")\n    else:\n        plt.plot([*range(1,len(train)+1)] , train , label = \"loss\")\n    plt.xticks(np.arange(0, len(train)+1, 5))\n    plt.legend(loc=\"upper left\")\n\n    plt.savefig(f'{name}.png')\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:16.221798Z","iopub.execute_input":"2023-06-17T13:07:16.222433Z","iopub.status.idle":"2023-06-17T13:07:16.235711Z","shell.execute_reply.started":"2023-06-17T13:07:16.222401Z","shell.execute_reply":"2023-06-17T13:07:16.234798Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import LambdaLR\n\n\ndef gradual_warmup(\n    optimizer: Optimizer,\n    num_warmup_epoch: int,\n    num_cycles: float = 0.5,\n    last_epoch: int = -1,\n):\n\n    def lr_lambda(current_epoch):\n        # Warmup\n        if current_epoch < num_warmup_epoch:\n            return float(current_epoch) / float(max(1, num_warmup_epoch))\n        # decadence\n        progress = float(current_epoch - num_warmup_epoch) / float(max(1, num_training_epoch - num_warmup_epoch))\n        \n        return max(\n            0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n        )\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:16.237271Z","iopub.execute_input":"2023-06-17T13:07:16.237921Z","iopub.status.idle":"2023-06-17T13:07:16.250663Z","shell.execute_reply.started":"2023-06-17T13:07:16.237889Z","shell.execute_reply":"2023-06-17T13:07:16.249568Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ROOT = Path(\"/kaggle/input/hubmap-hacking-the-human-vasculature\")\npolygon_file = \"polygons.jsonl\"\nimage_folder = \"train\"\nmetadata_file = \"tile_meta.csv\"\ntrain_valid_ratio = 0.9\n\nlabels = pd.read_json(ROOT / polygon_file , lines=True)\nid_list = get_valid_data(labels)\nmetadata = pd.read_csv(ROOT / metadata_file)\nimage_list = [str(i) for i  in (ROOT / image_folder).glob('*.tif') if i.stem in id_list]\ntrain_list = image_list[:int(len(image_list)*train_valid_ratio)]\nvalid_list = image_list[int(len(image_list)*train_valid_ratio):]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:16.252064Z","iopub.execute_input":"2023-06-17T13:07:16.252646Z","iopub.status.idle":"2023-06-17T13:07:21.561547Z","shell.execute_reply.started":"2023-06-17T13:07:16.252614Z","shell.execute_reply":"2023-06-17T13:07:21.560573Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# train_list = train_list[:10]","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:21.564946Z","iopub.execute_input":"2023-06-17T13:07:21.565351Z","iopub.status.idle":"2023-06-17T13:07:21.569168Z","shell.execute_reply.started":"2023-06-17T13:07:21.565313Z","shell.execute_reply":"2023-06-17T13:07:21.568243Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\nmodel = get_model_instance_segmentation(2).to(device)\nbatch_size = 4\nnum_epochs = 20\nwarmup_epoch = 5\noptimizer = torch.optim.SGD(model.parameters(), lr=0.0003, weight_decay=1e-5)\n_exp_name = \"mask-rcnn-model\"\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size=3 , gamma=0.1)\nlr_scheduler = gradual_warmup(optimizer , num_warmup_epoch=warmup_epoch)","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:21.570551Z","iopub.execute_input":"2023-06-17T13:07:21.571141Z","iopub.status.idle":"2023-06-17T13:07:28.450342Z","shell.execute_reply.started":"2023-06-17T13:07:21.571106Z","shell.execute_reply":"2023-06-17T13:07:28.449347Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n100%|██████████| 170M/170M [00:03<00:00, 55.4MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"import utils\ntrain_set = KidneyDataset(labels , metadata , train_list , get_transform(train=True))\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True , collate_fn=utils.collate_fn)\nvalid_set = KidneyDataset(labels , metadata , valid_list , get_transform(train=False))\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True , collate_fn=utils.collate_fn)\n# valid_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n# trainset[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:07:28.451819Z","iopub.execute_input":"2023-06-17T13:07:28.452162Z","iopub.status.idle":"2023-06-17T13:09:23.451365Z","shell.execute_reply.started":"2023-06-17T13:07:28.452128Z","shell.execute_reply":"2023-06-17T13:09:23.450077Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/input/hubmap-hacking-the-human-vasculature/train\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_28/2335065411.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/hubmap-hacking-the-human-vasculature/train\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom engine import train_one_epoch, evaluate\n\ntrain_loss = []\nvalid_bbox_acc = []\nvalid_segm_acc = []\nbest = 0\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    logger = train_one_epoch(model, optimizer, train_loader , device, epoch, print_freq=10)\n    train_loss.append(logger.meters['loss'].global_avg)\n    \n\n    # update the learning rate\n    lr_scheduler.step()\n    valid_logger = evaluate(model, valid_loader, device=device)\n    \n    bbox = valid_logger.coco_eval['bbox'].stats.mean()\n    segm = valid_logger.coco_eval['segm'].stats.mean()\n    \n    valid_bbox_acc.append(bbox)\n    valid_segm_acc.append(segm)\n    \n    if segm + bbox > best:\n        best = segm + bbox\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\")\n        print(f\"Update Best in epoch {epoch} , bbox = {bbox} , segm = {segm}\")\n        \ndraw_fig(train_loss , None , \"loss\")\ndraw_fig(valid_bbox_acc , valid_segm_acc , \"acc\")","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:09:23.453134Z","iopub.execute_input":"2023-06-17T13:09:23.453515Z","iopub.status.idle":"2023-06-17T13:09:43.959913Z","shell.execute_reply.started":"2023-06-17T13:09:23.453476Z","shell.execute_reply":"2023-06-17T13:09:43.958203Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch: [0]  [0/3]  eta: 0:00:16  lr: 0.000000  loss: 5.3642 (5.3642)  loss_classifier: 0.8237 (0.8237)  loss_box_reg: 0.1069 (0.1069)  loss_mask: 3.9622 (3.9622)  loss_objectness: 0.4515 (0.4515)  loss_rpn_box_reg: 0.0199 (0.0199)  time: 5.3899  data: 0.2408  max mem: 4529\nEpoch: [0]  [2/3]  eta: 0:00:02  lr: 0.000000  loss: 5.3642 (5.5009)  loss_classifier: 0.8237 (0.8149)  loss_box_reg: 0.1656 (0.2058)  loss_mask: 3.9622 (3.8485)  loss_objectness: 0.4515 (0.6046)  loss_rpn_box_reg: 0.0209 (0.0272)  time: 2.2599  data: 0.1954  max mem: 4918\nEpoch: [0] Total time: 0:00:06 (2.2620 s / it)\ncreating index...\nindex created!\nTest:  [ 0/41]  eta: 0:01:01  model_time: 0.7609 (0.7609)  evaluator_time: 0.6962 (0.6962)  time: 1.4962  data: 0.0378  max mem: 4918\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[1;32m     15\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m valid_logger \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m bbox \u001b[38;5;241m=\u001b[39m valid_logger\u001b[38;5;241m.\u001b[39mcoco_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     19\u001b[0m segm \u001b[38;5;241m=\u001b[39m valid_logger\u001b[38;5;241m.\u001b[39mcoco_eval[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mmean()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/kaggle/working/engine.py:102\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m res \u001b[38;5;241m=\u001b[39m {target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(): output \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)}\n\u001b[1;32m    101\u001b[0m evaluator_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 102\u001b[0m \u001b[43mcoco_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m evaluator_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m evaluator_time\n\u001b[1;32m    104\u001b[0m metric_logger\u001b[38;5;241m.\u001b[39mupdate(model_time\u001b[38;5;241m=\u001b[39mmodel_time, evaluator_time\u001b[38;5;241m=\u001b[39mevaluator_time)\n","File \u001b[0;32m/kaggle/working/coco_eval.py:40\u001b[0m, in \u001b[0;36mCocoEvaluator.update\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m     38\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39mcocoDt \u001b[38;5;241m=\u001b[39m coco_dt\n\u001b[1;32m     39\u001b[0m coco_eval\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mimgIds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img_ids)\n\u001b[0;32m---> 40\u001b[0m img_ids, eval_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_imgs[iou_type]\u001b[38;5;241m.\u001b[39mappend(eval_imgs)\n","File \u001b[0;32m/kaggle/working/coco_eval.py:191\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(imgs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(imgs):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m redirect_stdout(io\u001b[38;5;241m.\u001b[39mStringIO()):\n\u001b[0;32m--> 191\u001b[0m         \u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m imgs\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mimgIds, np\u001b[38;5;241m.\u001b[39masarray(imgs\u001b[38;5;241m.\u001b[39mevalImgs)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(imgs\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mareaRng), \u001b[38;5;28mlen\u001b[39m(imgs\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mimgIds))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pycocotools/cocoeval.py:154\u001b[0m, in \u001b[0;36mCOCOeval.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m evaluateImg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluateImg\n\u001b[1;32m    153\u001b[0m maxDet \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmaxDets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalImgs \u001b[38;5;241m=\u001b[39m [evaluateImg(imgId, catId, areaRng, maxDet)\n\u001b[1;32m    155\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m catId \u001b[38;5;129;01min\u001b[39;00m catIds\n\u001b[1;32m    156\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m areaRng \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mareaRng\n\u001b[1;32m    157\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m imgId \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mimgIds\n\u001b[1;32m    158\u001b[0m      ]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramsEval \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    160\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pycocotools/cocoeval.py:154\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    152\u001b[0m evaluateImg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluateImg\n\u001b[1;32m    153\u001b[0m maxDet \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmaxDets[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalImgs \u001b[38;5;241m=\u001b[39m [\u001b[43mevaluateImg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mareaRng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxDet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m catId \u001b[38;5;129;01min\u001b[39;00m catIds\n\u001b[1;32m    156\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m areaRng \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mareaRng\n\u001b[1;32m    157\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m imgId \u001b[38;5;129;01min\u001b[39;00m p\u001b[38;5;241m.\u001b[39mimgIds\n\u001b[1;32m    158\u001b[0m      ]\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paramsEval \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    160\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pycocotools/cocoeval.py:298\u001b[0m, in \u001b[0;36mCOCOeval.evaluateImg\u001b[0;34m(self, imgId, catId, aRng, maxDet)\u001b[0m\n\u001b[1;32m    296\u001b[0m             gtm[tind,m]     \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# set unmatched detections outside of area range to ignore\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m<\u001b[39maRng[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39maRng[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dt])\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dt)))\n\u001b[1;32m    299\u001b[0m dtIg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(dtIg, np\u001b[38;5;241m.\u001b[39mlogical_and(dtm\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mrepeat(a,T,\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# store results for given image and category\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pycocotools/cocoeval.py:298\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    296\u001b[0m             gtm[tind,m]     \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# set unmatched detections outside of area range to ignore\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m<\u001b[39maRng[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m>\u001b[39maRng[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dt])\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dt)))\n\u001b[1;32m    299\u001b[0m dtIg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(dtIg, np\u001b[38;5;241m.\u001b[39mlogical_and(dtm\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mrepeat(a,T,\u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# store results for given image and category\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# print(pred[0]['masks'].shape)\n# Image.fromarray(pred[0]['masks'][0,0].mul(255).byte().cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-06-17T13:09:43.961098Z","iopub.status.idle":"2023-06-17T13:09:43.962246Z","shell.execute_reply.started":"2023-06-17T13:09:43.961980Z","shell.execute_reply":"2023-06-17T13:09:43.962005Z"},"trusted":true},"execution_count":null,"outputs":[]}]}