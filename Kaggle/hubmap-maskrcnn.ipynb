{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random\nfrom pathlib import Path\nimport math\nimport cv2\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:37:17.698355Z","iopub.execute_input":"2023-06-16T12:37:17.698620Z","iopub.status.idle":"2023-06-16T12:37:22.566034Z","shell.execute_reply.started":"2023-06-16T12:37:17.698595Z","shell.execute_reply":"2023-06-16T12:37:22.563477Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pytorch/vision.git\n%cd vision\n# !git checkout v0.8.2\n\n!cp references/detection/utils.py ../\n!cp references/detection/transforms.py ../\n!cp references/detection/coco_eval.py ../\n!cp references/detection/engine.py ../\n!cp references/detection/coco_utils.py ../\n\n%cd ..\n!rm -r vision\n!pip install pycocotools\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:37:22.568008Z","iopub.execute_input":"2023-06-16T12:37:22.568673Z","iopub.status.idle":"2023-06-16T12:39:26.936554Z","shell.execute_reply.started":"2023-06-16T12:37:22.568631Z","shell.execute_reply":"2023-06-16T12:39:26.935319Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'vision'...\nremote: Enumerating objects: 339088, done.\u001b[K\nremote: Counting objects: 100% (55320/55320), done.\u001b[K\nremote: Compressing objects: 100% (1324/1324), done.\u001b[K\nremote: Total 339088 (delta 54547), reused 54617 (delta 53965), pack-reused 283768\u001b[K\nReceiving objects: 100% (339088/339088), 680.27 MiB | 22.80 MiB/s, done.\nResolving deltas: 100% (312398/312398), done.\n/kaggle/working/vision\n/kaggle/working\nCollecting pycocotools\n  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.6.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.23.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.0.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.39.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nBuilding wheels for collected packages: pycocotools\n  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp310-cp310-linux_x86_64.whl size=93512 sha256=104568fc429b02d8b6ba9374fa68c59de602116506bbfc185ff520c15c086d15\n  Stored in directory: /root/.cache/pip/wheels/58/e6/f9/f87c8f8be098b51b616871315318329cae12cdb618f4caac93\nSuccessfully built pycocotools\nInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"def get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n    # return model\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 1024\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:39:26.938564Z","iopub.execute_input":"2023-06-16T12:39:26.938964Z","iopub.status.idle":"2023-06-16T12:39:26.948236Z","shell.execute_reply.started":"2023-06-16T12:39:26.938919Z","shell.execute_reply":"2023-06-16T12:39:26.946440Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import transforms as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.PILToTensor())\n    transforms.append(T.ConvertImageDtype(torch.float))\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n        transforms.append(T.RandomZoomOut(side_range = (1,1.5)))\n        transforms.append(T.RandomPhotometricDistort())\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:39:26.951162Z","iopub.execute_input":"2023-06-16T12:39:26.951787Z","iopub.status.idle":"2023-06-16T12:39:26.971293Z","shell.execute_reply.started":"2023-06-16T12:39:26.951752Z","shell.execute_reply":"2023-06-16T12:39:26.970346Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_valid_data(labels):\n    valid_data = []\n    for i in range(len(labels)):\n        for j in range(len(labels.iloc[i]['annotations'])):\n            if labels.iloc[i]['annotations'][j]['type'] == \"blood_vessel\":\n                valid_data.append(labels.iloc[i]['id'])\n                break\n    \n    return valid_data\n\ndef gen_mask(annotations):\n    boxes = []\n    masks = []\n    # area = []\n    for i in range(len(annotations)):\n\n        if annotations[i]['type'] == \"blood_vessel\":\n            pts = np.array(annotations[i]['coordinates'])\n            # print(pts[0][0])\n            min_xy = np.min(pts, axis=1)[0]\n            max_xy = np.max(pts, axis=1)[0]\n\n            boxes += [np.concatenate((min_xy, max_xy), axis=0)]\n\n            mask = np.zeros((512,512), dtype=np.uint8)\n            cv2.fillPoly(mask, pts, 1)\n\n            # area.append((max_xy[0] - min_xy[0]) * (max_xy[1] - min_xy[1]))\n\n            masks += [mask]\n        # break\n    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n    masks = torch.as_tensor(masks, dtype=torch.uint8)\n    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n    return boxes , masks , area","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:39:26.974813Z","iopub.execute_input":"2023-06-16T12:39:26.975452Z","iopub.status.idle":"2023-06-16T12:39:26.985288Z","shell.execute_reply.started":"2023-06-16T12:39:26.975412Z","shell.execute_reply":"2023-06-16T12:39:26.984323Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class KidneyDataset(Dataset):\n    def __init__(self , labels , metadata , image_list , tfm):\n        self.labels = labels\n        self.matadata = metadata\n        self.image_list = image_list\n        self.tfm = tfm\n        \n        self.image_with_target = {}\n\n        image_path = Path(image_list[0]).parent\n        print(image_path)\n        \n        for idx , col in labels.iterrows():\n            \n            # if f\"{image_path}\\\\{col['id']}.tif\" in image_list:\n            if f\"{image_path}/{col['id']}.tif\" in image_list:\n                polygons = col['annotations']\n                # print(polygons[0])\n                boxes , masks , area = gen_mask(polygons)\n                iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n                \n                target = {}\n                target[\"boxes\"] = boxes\n                target[\"labels\"] = torch.ones((len(boxes),), dtype=torch.int64)\n                target[\"masks\"] = masks\n                target[\"image_id\"] = torch.tensor([idx])\n                target[\"area\"] = area\n                target[\"iscrowd\"] = iscrowd\n                self.image_with_target[col['id']] = target\n\n        # print()\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def __getitem__(self,idx):\n        \n        # id = self.image_list[idx].split('\\\\')[-1].split('.')[0]\n        id = self.image_list[idx].split('/')[-1].split('.')[0]\n        img = Image.open(self.image_list[idx])\n        target = self.image_with_target[id]\n\n\n        if self.tfm is not None:\n            img, target = self.tfm(img, target)\n        # print(target)\n        return img , target\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:39:26.986945Z","iopub.execute_input":"2023-06-16T12:39:26.987270Z","iopub.status.idle":"2023-06-16T12:39:26.999267Z","shell.execute_reply.started":"2023-06-16T12:39:26.987238Z","shell.execute_reply":"2023-06-16T12:39:26.998352Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ROOT = Path(\"/kaggle/input/hubmap-hacking-the-human-vasculature\")\npolygon_file = \"polygons.jsonl\"\nimage_folder = \"train\"\nmetadata_file = \"tile_meta.csv\"\ntrain_valid_ratio = 0.9\n\nlabels = pd.read_json(ROOT / polygon_file , lines=True)\nid_list = get_valid_data(labels)\nmetadata = pd.read_csv(ROOT / metadata_file)\nimage_list = [str(i) for i  in (ROOT / image_folder).glob('*.tif') if i.stem in id_list]\ntrain_list = image_list[:int(len(image_list)*train_valid_ratio)]\nvalid_list = image_list[int(len(image_list)*train_valid_ratio):]","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:39:27.000653Z","iopub.execute_input":"2023-06-16T12:39:27.001326Z","iopub.status.idle":"2023-06-16T12:39:32.466464Z","shell.execute_reply.started":"2023-06-16T12:39:27.001273Z","shell.execute_reply":"2023-06-16T12:39:32.465345Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef draw_fig(train , valid , name):\n    print(train)\n    print(valid)\n    plt.plot([*range(1,len(train)+1)] , train , label = \"training\")\n    \n    if valid != None:\n        plt.plot([*range(1,len(valid)+1)] , valid , label = \"validation\")\n\n    plt.xticks(np.arange(0, len(train)+1, 5))\n    plt.legend(loc=\"upper left\")\n\n    plt.savefig(f'{name}.png')\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T14:10:06.703117Z","iopub.execute_input":"2023-06-16T14:10:06.703504Z","iopub.status.idle":"2023-06-16T14:10:06.710426Z","shell.execute_reply.started":"2023-06-16T14:10:06.703471Z","shell.execute_reply":"2023-06-16T14:10:06.709349Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\nmodel = get_model_instance_segmentation(2).to(device)\nbatch_size = 4\nnum_epochs = 20\npatience = 10\noptimizer = torch.optim.SGD(model.parameters(), lr=0.0003, weight_decay=1e-5)\n_exp_name = \"mask-rcnn-model\"\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer , step_size=3 , gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:39:32.476006Z","iopub.execute_input":"2023-06-16T12:39:32.476891Z","iopub.status.idle":"2023-06-16T12:39:38.833552Z","shell.execute_reply.started":"2023-06-16T12:39:32.476841Z","shell.execute_reply":"2023-06-16T12:39:38.832578Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n100%|██████████| 170M/170M [00:02<00:00, 84.8MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"import utils\ntrain_set = KidneyDataset(labels , metadata , train_list , get_transform(train=True))\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True , collate_fn=utils.collate_fn)\nvalid_set = KidneyDataset(labels , metadata , valid_list , get_transform(train=False))\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True , collate_fn=utils.collate_fn)\n# valid_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n# trainset[0]\n","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:39:38.840004Z","iopub.execute_input":"2023-06-16T12:39:38.842265Z","iopub.status.idle":"2023-06-16T12:41:34.975465Z","shell.execute_reply.started":"2023-06-16T12:39:38.842227Z","shell.execute_reply":"2023-06-16T12:41:34.974452Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/input/hubmap-hacking-the-human-vasculature/train\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_29/2335065411.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n  boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/hubmap-hacking-the-human-vasculature/train\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom engine import train_one_epoch, evaluate\n\ntrain_loss = []\nvalid_bbox_acc = []\nvalid_segm_acc = []\nbest = 0\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    logger = train_one_epoch(model, optimizer, train_loader , device, epoch, print_freq=10)\n    train_loss.append(logger.meters['loss'].global_avg)\n    \n\n    # update the learning rate\n    lr_scheduler.step()\n    valid_logger = evaluate(model, valid_loader, device=device)\n    \n    bbox = valid_logger.coco_eval['bbox'].stats.mean()\n    segm = valid_logger.coco_eval['segm'].stats.mean()\n    \n    valid_bbox_acc.append(bbox)\n    valid_segm_acc.append(segm)\n    \n    if segm + bbox > best:\n        best = segm + bbox\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\")\n        print(f\"Update Best in epoch {epoch} , bbox = {bbox} , segm = {segm}\")\n        \ndraw_fig(train_loss , None , \"loss\")\ndraw_fig(valid_bbox_acc , valid_segm_acc , \"acc\")","metadata":{"execution":{"iopub.status.busy":"2023-06-16T14:08:22.878170Z","iopub.execute_input":"2023-06-16T14:08:22.879310Z","iopub.status.idle":"2023-06-16T14:09:26.355353Z","shell.execute_reply.started":"2023-06-16T14:08:22.879243Z","shell.execute_reply":"2023-06-16T14:09:26.354199Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"Epoch: [0]  [0/5]  eta: 0:00:01  lr: 0.000000  loss: 3.2330 (3.2330)  loss_classifier: 0.3299 (0.3299)  loss_box_reg: 0.0865 (0.0865)  loss_mask: 0.9912 (0.9912)  loss_objectness: 1.7419 (1.7419)  loss_rpn_box_reg: 0.0834 (0.0834)  time: 0.2726  data: 0.0224  max mem: 3309\nEpoch: [0]  [4/5]  eta: 0:00:00  lr: 0.000000  loss: 2.3476 (2.4173)  loss_classifier: 0.3351 (0.3413)  loss_box_reg: 0.1738 (0.1812)  loss_mask: 1.1108 (1.0504)  loss_objectness: 0.5550 (0.7919)  loss_rpn_box_reg: 0.0520 (0.0526)  time: 0.2713  data: 0.0203  max mem: 3309\nEpoch: [0] Total time: 0:00:01 (0.2717 s / it)\ncreating index...\nindex created!\nTest:  [ 0/82]  eta: 0:00:56  model_time: 0.3875 (0.3875)  evaluator_time: 0.2813 (0.2813)  time: 0.6881  data: 0.0185  max mem: 3309\nTest:  [81/82]  eta: 0:00:00  model_time: 0.3919 (0.3938)  evaluator_time: 0.2892 (0.2947)  time: 0.7345  data: 0.0202  max mem: 3309\nTest: Total time: 0:00:59 (0.7216 s / it)\nAveraged stats: model_time: 0.3919 (0.3938)  evaluator_time: 0.2892 (0.2947)\nAccumulating evaluation results...\nDONE (t=0.29s).\nAccumulating evaluation results...\nDONE (t=0.25s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.003\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.001\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.007\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.033\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.080\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.143\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.001\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.001\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.002\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.018\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.028\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.020\nUpdate Best in epoch 0 , bbox = 0.027397449806264452 , segm = 0.007878577822384754\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[71], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_exp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_best.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdate Best in epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , bbox = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , segm = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mdraw_fig\u001b[49m(train_loss , \u001b[38;5;28;01mNone\u001b[39;00m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m draw_fig(valid_bbox_acc , valid_segm_acc , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'draw_fig' is not defined"],"ename":"NameError","evalue":"name 'draw_fig' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# print(pred[0]['masks'].shape)\n# Image.fromarray(pred[0]['masks'][0,0].mul(255).byte().cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-06-16T12:51:59.007334Z","iopub.execute_input":"2023-06-16T12:51:59.007798Z","iopub.status.idle":"2023-06-16T12:51:59.020237Z","shell.execute_reply.started":"2023-06-16T12:51:59.007760Z","shell.execute_reply":"2023-06-16T12:51:59.019268Z"},"trusted":true},"execution_count":13,"outputs":[]}]}