{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "from torchvision.datasets import DatasetFolder, VisionDataset\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    return model\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    # transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[1313.2760,  286.0296, 1758.9568, 1237.1543],\n",
      "        [ 266.2980,  284.6473,  684.8087, 1221.7095],\n",
      "        [ 671.6182,  294.9633, 1066.5409, 1201.4760],\n",
      "        [ 987.9595,  344.5453, 1372.6268, 1221.9905],\n",
      "        [1667.1614,  508.9835, 1750.5774,  704.6721],\n",
      "        [1858.6136,  525.4602, 1927.5934,  702.2455],\n",
      "        [ 230.3424,  343.1954, 1266.8849, 1232.9656],\n",
      "        [   0.0000, 1125.3748,  295.1183, 1213.9221],\n",
      "        [ 663.1809,  323.5529, 1047.1290, 1204.9222],\n",
      "        [ 244.7778,  943.5069,  378.1649, 1201.7089],\n",
      "        [ 976.0917,  302.5929, 1349.7374, 1243.2539],\n",
      "        [ 252.3555,  411.7393,  694.8979, 1219.2700],\n",
      "        [1681.6613,  523.3652, 1804.3450,  710.5125],\n",
      "        [   0.0000, 1129.5797,  256.3327, 1212.4236],\n",
      "        [   0.0000, 1126.2727,  256.6265, 1206.2677],\n",
      "        [ 663.0552,  325.3092, 1049.7162, 1211.4883],\n",
      "        [1647.5883,  953.1304, 1699.2478, 1145.3979],\n",
      "        [ 439.1273,  515.6843,  517.7386,  619.7453],\n",
      "        [  14.1118, 1132.8143,  248.5149, 1215.7056],\n",
      "        [1707.2081,  602.6953, 1799.4307,  702.6761],\n",
      "        [  56.3050, 1137.2555,  249.4322, 1216.8649]],\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([18, 18, 18, 18,  1,  1, 18, 21, 20, 19, 20, 21,  1, 16,  3, 21, 18, 37,\n",
      "        18,  1, 27]), 'scores': tensor([0.9989, 0.9984, 0.9979, 0.9976, 0.8792, 0.7901, 0.4561, 0.3241, 0.2359,\n",
      "        0.1991, 0.1962, 0.1521, 0.1353, 0.1110, 0.1041, 0.0965, 0.0864, 0.0703,\n",
      "        0.0527, 0.0524, 0.0519], grad_fn=<IndexBackward0>), 'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], grad_fn=<UnsqueezeBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"test.jpg\").convert(\"RGB\")\n",
    "img = tfm(img)\n",
    "model = get_model_instance_segmentation(2)\n",
    "model.eval()\n",
    " \n",
    "pred = model(img.unsqueeze(0))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21, 1, 1331, 2000])\n"
     ]
    }
   ],
   "source": [
    "print(pred[0]['masks'].shape)\n",
    "# Image.fromarray(pred[0]['masks'][0,0].mul(255).byte().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
